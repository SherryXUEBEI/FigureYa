#!/usr/bin/env python3
"""
FigureYa RAG æ™ºèƒ½ç”Ÿç‰©åŒ»å­¦åˆ†æåŠ©æ‰‹
ä¿®å¤ç‰ˆæœ¬ - å¤„ç†SIGPIPEç­‰ä¿¡å·é—®é¢˜
"""

import json
import os
import re
import pandas as pd
from pathlib import Path
from typing import List, Dict, Tuple
import numpy as np
from collections import defaultdict
import signal
import sys

# ä¿®å¤SIGPIPEä¿¡å·é—®é¢˜
signal.signal(signal.SIGPIPE, signal.SIG_DFL)

class FigureYaRAGProcessor:
    """FigureYa RAGçŸ¥è¯†åº“å¤„ç†å™¨ - ä¿®å¤ç‰ˆ"""

    def __init__(self, figureya_path: str):
        self.figureya_path = Path(figureya_path)
        self.texts_path = self.figureya_path / "texts"
        self.chapters_json = self.figureya_path / "chapters.json"
        self.modules_path = self.figureya_path

        # çŸ¥è¯†åº“å­˜å‚¨
        self.knowledge_base = []
        self.module_index = {}
        self.technical_keywords = set()
        self.data_type_keywords = set()
        self.biology_keywords = set()

        # ç¼“å­˜å·²å¤„ç†çš„æ–‡ä»¶
        self._processed_files = set()

    def load_knowledge_base(self) -> List[Dict]:
        """åŠ è½½å¹¶å¤„ç†FigureYaçŸ¥è¯†åº“"""
        try:
            print("ğŸš€ å¼€å§‹åŠ è½½FigureYaçŸ¥è¯†åº“...")

            # åŠ è½½chaptersç´¢å¼•
            chapters = []
            if self.chapters_json.exists():
                try:
                    with open(self.chapters_json, 'r', encoding='utf-8') as f:
                        chapters = json.load(f)
                    print(f"ğŸ“š åŠ è½½äº† {len(chapters)} ä¸ªæ¨¡å—ç´¢å¼•")
                except (json.JSONDecodeError, IOError) as e:
                    print(f"âš ï¸ æ— æ³•åŠ è½½ç« èŠ‚ç´¢å¼•: {e}")
                    chapters = []

            # å¤„ç†æ‰€æœ‰æ–‡æœ¬æ–‡ä»¶
            text_files = list(self.texts_path.glob("*.txt"))
            print(f"ğŸ“„ å‘ç° {len(text_files)} ä¸ªæ–‡æœ¬æ–‡ä»¶")

            # é™åˆ¶å¤„ç†æ•°é‡ä»¥é¿å…å†…å­˜é—®é¢˜
            max_files = min(len(text_files), 200)  # é™åˆ¶ä¸º200ä¸ªæ–‡ä»¶
            text_files = text_files[:max_files]

            processed_count = 0
            for text_file in text_files:
                try:
                    if str(text_file) not in self._processed_files:
                        module_info = self._process_text_file(text_file, chapters)
                        if module_info:
                            self.knowledge_base.append(module_info)
                            self._processed_files.add(str(text_file))
                            processed_count += 1

                            # æ¯50ä¸ªæ–‡ä»¶è¾“å‡ºä¸€æ¬¡è¿›åº¦
                            if processed_count % 50 == 0:
                                print(f"ğŸ“Š å·²å¤„ç† {processed_count}/{len(text_files)} ä¸ªæ–‡ä»¶")

                except Exception as e:
                    print(f"âš ï¸ å¤„ç†æ–‡ä»¶ {text_file} æ—¶å‡ºé”™: {e}")
                    continue

            # æ„å»ºå…³é”®è¯ç´¢å¼•
            self._build_keyword_index()

            print(f"âœ… æˆåŠŸåŠ è½½ {len(self.knowledge_base)} ä¸ªæ¨¡å—çš„çŸ¥è¯†")
            return self.knowledge_base

        except Exception as e:
            print(f"âŒ åŠ è½½çŸ¥è¯†åº“æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return []

    def _process_text_file(self, text_file: Path, chapters: List[Dict]) -> Dict:
        """å¤„ç†å•ä¸ªæ–‡æœ¬æ–‡ä»¶"""
        try:
            # æå–æ¨¡å—ID
            module_id = text_file.stem

            # æŸ¥æ‰¾å¯¹åº”çš„ç« èŠ‚ä¿¡æ¯
            chapter_info = self._find_chapter_info(module_id, chapters)

            # å®‰å…¨è¯»å–æ–‡æœ¬å†…å®¹
            content = ""
            try:
                with open(text_file, 'r', encoding='utf-8') as f:
                    content = f.read()
            except UnicodeDecodeError:
                # å°è¯•å…¶ä»–ç¼–ç 
                try:
                    with open(text_file, 'r', encoding='gbk') as f:
                        content = f.read()
                except:
                    content = text_file.read_text(encoding='utf-8', errors='ignore')

            # æ£€æŸ¥å†…å®¹æ˜¯å¦æœ‰æ•ˆ
            if len(content.strip()) < 50:
                return None

            # æå–æ¨¡å—ä¿¡æ¯
            module_info = {
                "id": module_id,
                "file_path": str(text_file),
                "content": content,
                "chapter_info": chapter_info,
                "word_count": len(content.split()),
                "lines_count": len(content.split('\n')),
            }

            # æ™ºèƒ½åˆ†æå†…å®¹
            module_info.update(self._analyze_content(content))

            return module_info

        except Exception as e:
            print(f"âš ï¸ å¤„ç†æ¨¡å— {text_file} æ—¶å‡ºé”™: {e}")
            return None

    def _find_chapter_info(self, module_id: str, chapters: List[Dict]) -> Dict:
        """æŸ¥æ‰¾æ¨¡å—å¯¹åº”çš„ç« èŠ‚ä¿¡æ¯"""
        for chapter in chapters:
            if module_id in chapter.get("id", ""):
                return chapter
        return {}

    def _analyze_content(self, content: str) -> Dict:
        """æ™ºèƒ½åˆ†æå†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯"""
        try:
            analysis = {
                "title": self._extract_title(content),
                "description": self._extract_description(content),
                "input_data_types": self._extract_input_data_types(content),
                "output_types": self._extract_output_types(content),
                "technical_methods": self._extract_technical_methods(content),
                "biology_areas": self._extract_biology_areas(content),
                "complexity_level": self._assess_complexity(content),
                "code_snippets": self._extract_code_snippets(content),
                "key_parameters": self._extract_key_parameters(content),
            }

            return analysis
        except Exception as e:
            print(f"âš ï¸ åˆ†æå†…å®¹æ—¶å‡ºé”™: {e}")
            return {
                "title": "æœªçŸ¥æ ‡é¢˜",
                "description": "æš‚æ— æè¿°",
                "input_data_types": [],
                "output_types": [],
                "technical_methods": [],
                "biology_areas": [],
                "complexity_level": "ä¸­çº§",
                "code_snippets": [],
                "key_parameters": []
            }

    def _extract_title(self, content: str) -> str:
        """æå–æ ‡é¢˜"""
        try:
            lines = content.split('\n')
            for line in lines[:10]:  # åªæ£€æŸ¥å‰10è¡Œ
                line = line.strip()
                if line and len(line) < 100:  # åˆç†çš„æ ‡é¢˜é•¿åº¦
                    # åŒ¹é…å¯èƒ½çš„æ ‡é¢˜æ¨¡å¼
                    if re.match(r'^#{1,3}\s+', line) or \
                       re.match(r'^[A-Z][^.!?]*[.!?]?$', line) or \
                       "FigureYa" in line:
                        return line
            return "æœªçŸ¥æ ‡é¢˜"
        except:
            return "æœªçŸ¥æ ‡é¢˜"

    def _extract_description(self, content: str) -> str:
        """æå–æè¿°ä¿¡æ¯"""
        try:
            lines = content.split('\n')
            description_lines = []

            for i, line in enumerate(lines):
                line = line.strip()
                # æŸ¥æ‰¾æè¿°æ€§å…³é”®è¯
                desc_keywords = ["éœ€æ±‚æè¿°", "åº”ç”¨åœºæ™¯", "åŠŸèƒ½", "åˆ†æ", "å¯è§†åŒ–", "å±•ç¤º"]
                if any(keyword in line for keyword in desc_keywords):
                    # æ”¶é›†æ¥ä¸‹æ¥çš„å‡ è¡Œä½œä¸ºæè¿°
                    j = i + 1
                    while j < len(lines) and j < i + 5:
                        next_line = lines[j].strip()
                        if next_line and not next_line.startswith('#'):
                            description_lines.append(next_line)
                        j += 1
                    break

            return ' '.join(description_lines[:3]) if description_lines else "æš‚æ— æè¿°"
        except:
            return "æš‚æ— æè¿°"

    def _extract_input_data_types(self, content: str) -> List[str]:
        """æå–è¾“å…¥æ•°æ®ç±»å‹"""
        try:
            data_types = []

            # æ•°æ®ç±»å‹å…³é”®è¯æ˜ å°„
            data_type_patterns = {
                "RNA-seq": ["RNA-seq", "RNAseq", "è½¬å½•ç»„"],
                "DNA-seq": ["DNA-seq", "DNAseq", "åŸºå› ç»„", "å…¨åŸºå› ç»„"],
                "ChIP-seq": ["ChIP-seq", "ChIPseq", "æŸ“è‰²è´¨å…ç–«æ²‰æ·€"],
                "å•ç»†èƒ": ["å•ç»†èƒ", "scRNA-seq", "single cell", "10x"],
                "è›‹ç™½è´¨ç»„": ["è›‹ç™½è´¨ç»„", "proteomics", "è´¨è°±"],
                "ä»£è°¢ç»„": ["ä»£è°¢ç»„", "metabolomics"],
                "ä¸´åºŠæ•°æ®": ["ä¸´åºŠ", "TCGA", "GEO", "ç—…äºº", "æ ·æœ¬"],
                "è¡¨è¾¾çŸ©é˜µ": ["è¡¨è¾¾çŸ©é˜µ", "expression matrix", "FPKM", "TPM"],
                "çªå˜æ•°æ®": ["çªå˜", "mutation", "SNV", "CNV"],
                "ç”Ÿå­˜æ•°æ®": ["ç”Ÿå­˜", "survival", "OS", "PFS"],
            }

            content_lower = content.lower()
            for data_type, keywords in data_type_patterns.items():
                if any(keyword in content_lower for keyword in keywords):
                    data_types.append(data_type)

            return list(set(data_types))
        except:
            return []

    def _extract_output_types(self, content: str) -> List[str]:
        """æå–è¾“å‡ºç±»å‹"""
        try:
            output_types = []

            output_patterns = {
                "çƒ­å›¾": ["çƒ­å›¾", "heatmap", "èšç±»å›¾"],
                "ç«å±±å›¾": ["ç«å±±å›¾", "volcano", "å·®å¼‚è¡¨è¾¾"],
                "PCAå›¾": ["PCA", "ä¸»æˆåˆ†", "é™ç»´"],
                "ç”Ÿå­˜æ›²çº¿": ["ç”Ÿå­˜æ›²çº¿", "survival", "Kaplan"],
                "ç®±çº¿å›¾": ["ç®±çº¿å›¾", "boxplot", "violin"],
                "æ•£ç‚¹å›¾": ["æ•£ç‚¹å›¾", "scatter", "correlation"],
                "ç½‘ç»œå›¾": ["ç½‘ç»œ", "network", "PPI", "äº’ä½œ"],
                "åŸºå› ç»„æµè§ˆå™¨": ["IGV", "genome browser", "åŸºå› ç»„è§†å›¾"],
                "ç»Ÿè®¡è¡¨æ ¼": ["è¡¨æ ¼", "table", "ç»Ÿè®¡"],
            }

            content_lower = content.lower()
            for output_type, keywords in output_patterns.items():
                if any(keyword in content_lower for keyword in keywords):
                    output_types.append(output_type)

            return list(set(output_types))
        except:
            return []

    def _extract_technical_methods(self, content: str) -> List[str]:
        """æå–æŠ€æœ¯æ–¹æ³•"""
        try:
            methods = []

            method_patterns = {
                "å·®å¼‚è¡¨è¾¾åˆ†æ": ["å·®å¼‚è¡¨è¾¾", "differential expression", "DEG", "limma"],
                "èšç±»åˆ†æ": ["èšç±»", "clustering", "hierarchical", "k-means"],
                "ç”Ÿå­˜åˆ†æ": ["ç”Ÿå­˜åˆ†æ", "cox", "logrank", "kaplan"],
                "é€šè·¯åˆ†æ": ["é€šè·¯", "pathway", "GSEA", "å¯Œé›†"],
                "è´¨é‡æ§åˆ¶": ["è´¨æ§", "QC", "è´¨é‡æ§åˆ¶", "quality"],
                "æ ‡å‡†åŒ–": ["æ ‡å‡†åŒ–", "normalization", "FPKM", "TPM"],
                "ä¸»æˆåˆ†åˆ†æ": ["PCA", "ä¸»æˆåˆ†", "principal component"],
                "ç½‘ç»œåˆ†æ": ["ç½‘ç»œåˆ†æ", "network", "PPI", "STRING"],
                "motifåˆ†æ": ["motif", "TF", "è½¬å½•å› å­"],
            }

            content_lower = content.lower()
            for method, keywords in method_patterns.items():
                if any(keyword in content_lower for keyword in keywords):
                    methods.append(method)

            return list(set(methods))
        except:
            return []

    def _extract_biology_areas(self, content: str) -> List[str]:
        """æå–ç”Ÿç‰©å­¦é¢†åŸŸ"""
        try:
            areas = []

            area_patterns = {
                "ç™Œç—‡ç ”ç©¶": ["ç™Œç—‡", "cancer", "tumor", "TCGA"],
                "å…ç–«å­¦": ["å…ç–«", "immune", "Tç»†èƒ", "Bç»†èƒ"],
                "ç¥ç»ç§‘å­¦": ["ç¥ç»", "neuron", "brain"],
                "å¿ƒè¡€ç®¡": ["å¿ƒè„", "å¿ƒè¡€ç®¡", "cardiovascular"],
                "ä»£è°¢ç–¾ç—…": ["ä»£è°¢", "ç³–å°¿ç—…", "obesity"],
                "æ„ŸæŸ“æ€§ç–¾ç—…": ["æ„ŸæŸ“", "ç—…æ¯’", "ç»†èŒ"],
                "å‘è‚²ç”Ÿç‰©å­¦": ["å‘è‚²", "èƒšèƒ", "stem cell"],
                "è¯ç‰©ç ”ç©¶": ["è¯ç‰©", "drug", "treatment"],
            }

            content_lower = content.lower()
            for area, keywords in area_patterns.items():
                if any(keyword in content_lower for keyword in keywords):
                    areas.append(area)

            return list(set(areas))
        except:
            return []

    def _assess_complexity(self, content: str) -> str:
        """è¯„ä¼°å¤æ‚åº¦"""
        try:
            complexity_indicators = {
                "é«˜çº§": ["é«˜çº§", "å¤æ‚", "å¤šæ­¥éª¤", "ç»¼åˆ"],
                "ä¸­çº§": ["ä¸­çº§", "å¸¸è§„", "æ ‡å‡†"],
                "åˆçº§": ["ç®€å•", "åŸºç¡€", "å…¥é—¨", "å¿«é€Ÿ"],
            }

            content_lower = content.lower()
            for level, keywords in complexity_indicators.items():
                if any(keyword in content_lower for keyword in keywords):
                    return level

            # åŸºäºä»£ç å¤æ‚åº¦åˆ¤æ–­
            code_complexity = len([line for line in content.split('\n')
                                  if line.strip().startswith(('library(', 'require('))])

            if code_complexity > 10:
                return "é«˜çº§"
            elif code_complexity > 5:
                return "ä¸­çº§"
            else:
                return "åˆçº§"
        except:
            return "ä¸­çº§"

    def _extract_code_snippets(self, content: str) -> List[str]:
        """æå–ä»£ç ç‰‡æ®µ"""
        try:
            code_snippets = []

            # æå–Rä»£ç å—
            r_code_pattern = r'```{r[^}]*}(.*?)```'
            matches = re.findall(r_code_pattern, content, re.DOTALL)

            for match in matches:
                # æ¸…ç†ä»£ç ç‰‡æ®µ
                clean_code = re.sub(r'\n\s+', '\n', match.strip())
                if len(clean_code) > 20:  # åªä¿ç•™æœ‰æ„ä¹‰çš„ä»£ç ç‰‡æ®µ
                    code_snippets.append(clean_code)

            return code_snippets[:3]  # æœ€å¤šè¿”å›3ä¸ªä»£ç ç‰‡æ®µ
        except:
            return []

    def _extract_key_parameters(self, content: str) -> List[str]:
        """æå–å…³é”®å‚æ•°"""
        try:
            parameters = []

            # æŸ¥æ‰¾å‚æ•°è®¾ç½®ç›¸å…³å†…å®¹
            param_patterns = [
                r'(\w+)\s*=\s*["\']?\w+["\']?',  # å˜é‡èµ‹å€¼
                r'(\w+)\s*=\s*\d+',             # æ•°å€¼å‚æ•°
                r'(\w+)\s*=\s*TRUE|FALSE',      # é€»è¾‘å‚æ•°
            ]

            content_lower = content.lower()
            for pattern in param_patterns:
                try:
                    matches = re.findall(pattern, content)
                    parameters.extend(matches)
                except:
                    continue

            # æå–å¸¸è§çš„ç”Ÿç‰©ä¿¡æ¯å­¦å‚æ•°
            common_params = ["pvalue", "adj.P.Val", "logFC", "FDR", "threshold", "min_size"]
            for param in common_params:
                if param in content_lower:
                    parameters.append(param)

            return list(set(parameters))[:10]  # æœ€å¤šè¿”å›10ä¸ªå‚æ•°
        except:
            return []

    def _build_keyword_index(self):
        """æ„å»ºå…³é”®è¯ç´¢å¼•"""
        try:
            for module in self.knowledge_base:
                # æŠ€æœ¯å…³é”®è¯
                self.technical_keywords.update(module.get("technical_methods", []))
                # æ•°æ®ç±»å‹å…³é”®è¯
                self.data_type_keywords.update(module.get("input_data_types", []))
                # ç”Ÿç‰©å­¦å…³é”®è¯
                self.biology_keywords.update(module.get("biology_areas", []))

            print(f"ğŸ”‘ å»ºç«‹å…³é”®è¯ç´¢å¼•:")
            print(f"   æŠ€æœ¯å…³é”®è¯: {len(self.technical_keywords)} ä¸ª")
            print(f"   æ•°æ®ç±»å‹å…³é”®è¯: {len(self.data_type_keywords)} ä¸ª")
            print(f"   ç”Ÿç‰©å­¦å…³é”®è¯: {len(self.biology_keywords)} ä¸ª")
        except Exception as e:
            print(f"âš ï¸ æ„å»ºå…³é”®è¯ç´¢å¼•æ—¶å‡ºé”™: {e}")

    def search_modules(self, query: str, top_k: int = 5) -> List[Dict]:
        """åŸºäºå…³é”®è¯æœç´¢ç›¸å…³æ¨¡å—"""
        try:
            if not self.knowledge_base:
                return []

            query_lower = query.lower()
            scored_modules = []

            for module in self.knowledge_base:
                score = self._calculate_relevance_score(query_lower, module)
                if score > 0:
                    module_copy = module.copy()
                    module_copy["relevance_score"] = score
                    scored_modules.append(module_copy)

            # æŒ‰ç›¸å…³æ€§æ’åº
            scored_modules.sort(key=lambda x: x["relevance_score"], reverse=True)

            return scored_modules[:top_k]
        except Exception as e:
            print(f"âš ï¸ æœç´¢æ¨¡å—æ—¶å‡ºé”™: {e}")
            return []

    def _calculate_relevance_score(self, query: str, module: Dict) -> float:
        """è®¡ç®—æ¨¡å—ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§å¾—åˆ†"""
        try:
            score = 0.0
            content_lower = module.get("content", "").lower()

            # å®Œæ•´åŒ¹é…å¾—åˆ†æ›´é«˜
            if query in content_lower:
                score += 10.0

            # å…³é”®è¯åŒ¹é…
            query_words = query.split()

            # æŠ€æœ¯æ–¹æ³•åŒ¹é…
            for method in module.get("technical_methods", []):
                if any(word in method.lower() for word in query_words):
                    score += 3.0

            # æ•°æ®ç±»å‹åŒ¹é…
            for data_type in module.get("input_data_types", []):
                if any(word in data_type.lower() for word in query_words):
                    score += 2.5

            # è¾“å‡ºç±»å‹åŒ¹é…
            for output_type in module.get("output_types", []):
                if any(word in output_type.lower() for word in query_words):
                    score += 2.0

            # ç”Ÿç‰©å­¦é¢†åŸŸåŒ¹é…
            for area in module.get("biology_areas", []):
                if any(word in area.lower() for word in query_words):
                    score += 1.5

            # æ ‡é¢˜åŒ¹é…
            title = module.get("title", "").lower()
            if any(word in title for word in query_words):
                score += 4.0

            return score
        except:
            return 0.0

    def get_module_recommendations(self, data_type: str, analysis_goal: str) -> List[Dict]:
        """åŸºäºæ•°æ®ç±»å‹å’Œåˆ†æç›®æ ‡æ¨èæ¨¡å—"""
        query = f"{data_type} {analysis_goal}"
        return self.search_modules(query, top_k=3)

    def export_knowledge_base(self, output_file: str = "figureya_knowledge_base.json"):
        """å¯¼å‡ºçŸ¥è¯†åº“ä¸ºJSONæ–‡ä»¶"""
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(self.knowledge_base, f, ensure_ascii=False, indent=2)
            print(f"âœ… çŸ¥è¯†åº“å·²å¯¼å‡ºåˆ°: {output_file}")
        except Exception as e:
            print(f"âš ï¸ å¯¼å‡ºçŸ¥è¯†åº“æ—¶å‡ºé”™: {e}")

    def generate_summary_report(self) -> str:
        """ç”ŸæˆçŸ¥è¯†åº“æ‘˜è¦æŠ¥å‘Š"""
        try:
            total_modules = len(self.knowledge_base)

            # ç»Ÿè®¡å„ç±»æ¨¡å—æ•°é‡
            data_type_counts = defaultdict(int)
            method_counts = defaultdict(int)
            complexity_counts = defaultdict(int)

            for module in self.knowledge_base:
                for data_type in module.get("input_data_types", []):
                    data_type_counts[data_type] += 1
                for method in module.get("technical_methods", []):
                    method_counts[method] += 1
                complexity = module.get("complexity_level", "æœªçŸ¥")
                complexity_counts[complexity] += 1

            report = f"""
# FigureYa çŸ¥è¯†åº“æ‘˜è¦æŠ¥å‘Š

## ğŸ“Š åŸºæœ¬ç»Ÿè®¡
- **æ€»æ¨¡å—æ•°**: {total_modules}
- **æŠ€æœ¯æ–¹æ³•ç§ç±»**: {len(method_counts)}
- **æ•°æ®ç±»å‹ç§ç±»**: {len(data_type_counts)}
- **å¤æ‚åº¦åˆ†å¸ƒ**: {dict(complexity_counts)}

## ğŸ”¥ çƒ­é—¨æŠ€æœ¯æ–¹æ³•
"""

            # æŒ‰ä½¿ç”¨é¢‘ç‡æ’åº
            top_methods = sorted(method_counts.items(), key=lambda x: x[1], reverse=True)[:10]
            for method, count in top_methods:
                report += f"- **{method}**: {count} ä¸ªæ¨¡å—\n"

            report += "\n## ğŸ“ˆ ä¸»è¦æ•°æ®ç±»å‹\n"
            top_data_types = sorted(data_type_counts.items(), key=lambda x: x[1], reverse=True)[:10]
            for data_type, count in top_data_types:
                report += f"- **{data_type}**: {count} ä¸ªæ¨¡å—\n"

            return report
        except Exception as e:
            print(f"âš ï¸ ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºé”™: {e}")
            return "# ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºé”™"

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.knowledge_base.clear()
        self.technical_keywords.clear()
        self.data_type_keywords.clear()
        self.biology_keywords.clear()
        self._processed_files.clear()


def main():
    """ä¸»å‡½æ•°"""
    try:
        # ä¿®å¤SIGPIPE
        signal.signal(signal.SIGPIPE, signal.SIG_DFL)

        print("ğŸ§  FigureYa RAG æ™ºèƒ½ç”Ÿç‰©åŒ»å­¦åˆ†æåŠ©æ‰‹")
        print("=" * 50)

        # åˆå§‹åŒ–å¤„ç†å™¨
        processor = FigureYaRAGProcessor("/Users/mypro/Downloads/FigureYa")

        # åŠ è½½çŸ¥è¯†åº“
        knowledge_base = processor.load_knowledge_base()

        if not knowledge_base:
            print("âŒ æ— æ³•åŠ è½½çŸ¥è¯†åº“ï¼Œç¨‹åºé€€å‡º")
            return

        # ç¤ºä¾‹æŸ¥è¯¢
        test_queries = [
            "å·®å¼‚è¡¨è¾¾åˆ†æ RNA-seq",
            "ç”Ÿå­˜åˆ†æ ä¸´åºŠæ•°æ®",
            "å•ç»†èƒ è´¨é‡æ§åˆ¶",
            "PCA é™ç»´",
            "çƒ­å›¾ èšç±»"
        ]

        print("\nğŸ” æµ‹è¯•æŸ¥è¯¢ç»“æœ:")
        for query in test_queries:
            try:
                results = processor.search_modules(query, top_k=3)
                print(f"\næŸ¥è¯¢: {query}")
                for i, result in enumerate(results, 1):
                    title = result.get("title", "æœªçŸ¥æ ‡é¢˜")
                    score = result.get("relevance_score", 0)
                    print(f"  {i}. {title} (ç›¸å…³æ€§: {score:.1f})")
            except Exception as e:
                print(f"  âš ï¸ æŸ¥è¯¢ '{query}' æ—¶å‡ºé”™: {e}")

        # å¯¼å‡ºçŸ¥è¯†åº“
        try:
            processor.export_knowledge_base()
        except Exception as e:
            print(f"âš ï¸ å¯¼å‡ºçŸ¥è¯†åº“æ—¶å‡ºé”™: {e}")

        # ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
        try:
            report = processor.generate_summary_report()
            with open("figureya_summary_report_fixed.md", "w", encoding="utf-8") as f:
                f.write(report)
            print("\nğŸ“„ æ‘˜è¦æŠ¥å‘Šå·²ç”Ÿæˆ: figureya_summary_report_fixed.md")
        except Exception as e:
            print(f"âš ï¸ ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºé”™: {e}")

        # æ¸…ç†èµ„æº
        processor.cleanup()

    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œç¨‹åºé€€å‡º")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
    finally:
        print("\nâœ… ç¨‹åºæ‰§è¡Œå®Œæ¯•")


if __name__ == "__main__":
    main()