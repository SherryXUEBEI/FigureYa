#!/usr/bin/env python3
"""
çœŸæ­£æ™ºèƒ½çš„FigureYa RAGç³»ç»Ÿ
é›†æˆç°ä»£LLM APIå’Œå‘é‡æ•°æ®åº“
"""

import os
import json
import requests
import time
from pathlib import Path
from typing import List, Dict, Tuple
import numpy as np
from dataclasses import dataclass
import pickle
import hashlib

@dataclass
class RAGConfig:
    """RAGç³»ç»Ÿé…ç½®"""
    # OpenAIé…ç½®
    openai_api_key: str = ""
    openai_model: str = "gpt-3.5-turbo"
    embedding_model: str = "text-embedding-ada-002"

    # æœ¬åœ°æ¨¡å‹é…ç½®ï¼ˆå¤‡é€‰ï¼‰
    local_embedding_model: str = "all-MiniLM-L6-v2"
    local_llm_model: str = "llama3-8b"

    # å‘é‡æ•°æ®åº“é…ç½®
    vector_store_path: str = "figureya_vector_store.pkl"
    chunk_size: int = 500
    chunk_overlap: int = 50

    # æ£€ç´¢é…ç½®
    top_k: int = 5
    similarity_threshold: float = 0.7

class SmartFigureYaRAG:
    """çœŸæ­£æ™ºèƒ½çš„FigureYa RAGç³»ç»Ÿ"""

    def __init__(self, config: RAGConfig = None):
        self.config = config or RAGConfig()
        self.figureya_path = Path("/Users/mypro/Downloads/FigureYa")
        self.vector_store = {}
        self.text_chunks = []
        self.embeddings = None

        # æ£€æŸ¥APIå¯†é’¥
        self.api_available = self._check_apis()

    def _check_apis(self) -> Dict[str, bool]:
        """æ£€æŸ¥å¯ç”¨çš„API"""
        apis = {
            "openai": bool(self.config.openai_api_key),
            "sentence_transformer": False,
            "chromadb": False,
            "faiss": False
        }

        # æ£€æŸ¥æœ¬åœ°åŒ…
        try:
            import sentence_transformers
            apis["sentence_transformer"] = True
        except ImportError:
            pass

        try:
            import chromadb
            apis["chromadb"] = True
        except ImportError:
            pass

        try:
            import faiss
            apis["faiss"] = True
        except ImportError:
            pass

        return apis

    def load_knowledge_base(self):
        """åŠ è½½å¹¶å¤„ç†FigureYaçŸ¥è¯†åº“"""
        print("ğŸ§  æ­£åœ¨æ„å»ºæ™ºèƒ½çŸ¥è¯†åº“...")

        # åŠ è½½æ–‡æœ¬æ–‡ä»¶
        texts_path = self.figureya_path / "texts"
        text_files = list(texts_path.glob("*.txt"))[:50]  # é™åˆ¶æ•°é‡ä»¥èŠ‚çœå†…å­˜

        all_chunks = []
        for text_file in text_files:
            try:
                with open(text_file, 'r', encoding='utf-8') as f:
                    content = f.read()

                # æ™ºèƒ½åˆ†å—
                chunks = self._smart_chunk_text(content, str(text_file))
                all_chunks.extend(chunks)

            except Exception as e:
                print(f"âš ï¸ å¤„ç†æ–‡ä»¶ {text_file} æ—¶å‡ºé”™: {e}")

        self.text_chunks = all_chunks
        print(f"ğŸ“š å·²å¤„ç† {len(all_chunks)} ä¸ªæ–‡æœ¬å—")

        # ç”Ÿæˆembeddings
        self._generate_embeddings()

    def _smart_chunk_text(self, text: str, source: str) -> List[Dict]:
        """æ™ºèƒ½æ–‡æœ¬åˆ†å—"""
        chunks = []

        # æŒ‰æ®µè½åˆ†å—
        paragraphs = text.split('\n\n')
        current_chunk = ""

        for paragraph in paragraphs:
            if len(current_chunk) + len(paragraph) < self.config.chunk_size:
                current_chunk += paragraph + "\n\n"
            else:
                if current_chunk.strip():
                    chunks.append({
                        "text": current_chunk.strip(),
                        "source": source,
                        "word_count": len(current_chunk.split())
                    })
                current_chunk = paragraph + "\n\n"

        # æ·»åŠ æœ€åä¸€ä¸ªå—
        if current_chunk.strip():
            chunks.append({
                "text": current_chunk.strip(),
                "source": source,
                "word_count": len(current_chunk.split())
            })

        return chunks

    def _generate_embeddings(self):
        """ç”Ÿæˆæ–‡æœ¬embeddings"""
        print("ğŸ” æ­£åœ¨ç”Ÿæˆå‘é‡è¡¨ç¤º...")

        if self.api_available["openai"] and self.config.openai_api_key:
            self._generate_openai_embeddings()
        elif self.api_available["sentence_transformer"]:
            self._generate_local_embeddings()
        else:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„embeddingæ¨¡å‹")
            return

        print(f"âœ… å·²ç”Ÿæˆ {len(self.embeddings)} ä¸ªå‘é‡")

    def _generate_openai_embeddings(self):
        """ä½¿ç”¨OpenAIç”Ÿæˆembeddings"""
        texts = [chunk["text"] for chunk in self.text_chunks]

        headers = {
            "Authorization": f"Bearer {self.config.openai_api_key}",
            "Content-Type": "application/json"
        }

        # åˆ†æ‰¹å¤„ç†ä»¥é¿å…APIé™åˆ¶
        batch_size = 100
        all_embeddings = []

        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i+batch_size]

            data = {
                "input": batch_texts,
                "model": self.config.embedding_model
            }

            try:
                response = requests.post(
                    "https://api.openai.com/v1/embeddings",
                    headers=headers,
                    json=data,
                    timeout=30
                )

                if response.status_code == 200:
                    result = response.json()
                    batch_embeddings = [item["embedding"] for item in result["data"]]
                    all_embeddings.extend(batch_embeddings)
                    print(f"  ğŸ“Š å·²å¤„ç† {min(i+batch_size, len(texts))}/{len(texts)} ä¸ªæ–‡æœ¬å—")
                else:
                    print(f"  âš ï¸ APIè¯·æ±‚å¤±è´¥: {response.status_code}")

            except Exception as e:
                print(f"  âŒ ç”Ÿæˆembeddingæ—¶å‡ºé”™: {e}")

        self.embeddings = np.array(all_embeddings)

    def _generate_local_embeddings(self):
        """ä½¿ç”¨æœ¬åœ°æ¨¡å‹ç”Ÿæˆembeddings"""
        try:
            from sentence_transformers import SentenceTransformer

            print("  ğŸ”„ åŠ è½½æœ¬åœ°embeddingæ¨¡å‹...")
            model = SentenceTransformer(self.config.local_embedding_model)

            texts = [chunk["text"] for chunk in self.text_chunks]
            self.embeddings = model.encode(texts, show_progress_bar=True)

        except Exception as e:
            print(f"âŒ æœ¬åœ°embeddingç”Ÿæˆå¤±è´¥: {e}")

    def save_vector_store(self):
        """ä¿å­˜å‘é‡å­˜å‚¨"""
        try:
            vector_store_data = {
                "chunks": self.text_chunks,
                "embeddings": self.embeddings,
                "config": self.config.__dict__,
                "timestamp": time.time()
            }

            with open(self.config.vector_store_path, 'wb') as f:
                pickle.dump(vector_store_data, f)

            print(f"ğŸ’¾ å‘é‡å­˜å‚¨å·²ä¿å­˜åˆ°: {self.config.vector_store_path}")

        except Exception as e:
            print(f"âš ï¸ ä¿å­˜å‘é‡å­˜å‚¨æ—¶å‡ºé”™: {e}")

    def load_vector_store(self):
        """åŠ è½½å‘é‡å­˜å‚¨"""
        try:
            if os.path.exists(self.config.vector_store_path):
                with open(self.config.vector_store_path, 'rb') as f:
                    data = pickle.load(f)

                self.text_chunks = data["chunks"]
                self.embeddings = data["embeddings"]
                print(f"ğŸ“‚ å·²åŠ è½½å‘é‡å­˜å‚¨: {len(self.text_chunks)} ä¸ªæ–‡æœ¬å—")
                return True
        except Exception as e:
            print(f"âš ï¸ åŠ è½½å‘é‡å­˜å‚¨æ—¶å‡ºé”™: {e}")

        return False

    def search(self, query: str, top_k: int = None) -> List[Dict]:
        """æ™ºèƒ½è¯­ä¹‰æœç´¢"""
        if not hasattr(self, 'embeddings') or self.embeddings is None:
            return []

        top_k = top_k or self.config.top_k

        # ç”ŸæˆæŸ¥è¯¢çš„embedding
        query_embedding = self._get_query_embedding(query)
        if query_embedding is None:
            return []

        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = self._compute_similarity(query_embedding)

        # è·å–top-kç»“æœ
        top_indices = np.argsort(similarities)[::-1][:top_k]

        results = []
        for idx in top_indices:
            if similarities[idx] >= self.config.similarity_threshold:
                chunk = self.text_chunks[idx]
                results.append({
                    "text": chunk["text"],
                    "source": chunk["source"],
                    "similarity": float(similarities[idx]),
                    "word_count": chunk["word_count"]
                })

        return results

    def _get_query_embedding(self, query: str) -> np.ndarray:
        """è·å–æŸ¥è¯¢çš„embedding"""
        if self.api_available["openai"] and self.config.openai_api_key:
            return self._get_openai_embedding(query)
        elif self.api_available["sentence_transformer"]:
            return self._get_local_embedding(query)
        else:
            return None

    def _get_openai_embedding(self, text: str) -> np.ndarray:
        """ä½¿ç”¨OpenAIè·å–embedding"""
        headers = {
            "Authorization": f"Bearer {self.config.openai_api_key}",
            "Content-Type": "application/json"
        }

        data = {
            "input": text,
            "model": self.config.embedding_model
        }

        try:
            response = requests.post(
                "https://api.openai.com/v1/embeddings",
                headers=headers,
                json=data,
                timeout=30
            )

            if response.status_code == 200:
                result = response.json()
                return np.array(result["data"][0]["embedding"])
            else:
                print(f"âš ï¸ OpenAI APIè¯·æ±‚å¤±è´¥: {response.status_code}")
                return None

        except Exception as e:
            print(f"âŒ OpenAI embeddingè¯·æ±‚å¤±è´¥: {e}")
            return None

    def _get_local_embedding(self, text: str) -> np.ndarray:
        """ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·å–embedding"""
        try:
            from sentence_transformers import SentenceTransformer

            if not hasattr(self, 'local_model'):
                self.local_model = SentenceTransformer(self.config.local_embedding_model)

            return self.local_model.encode(text)

        except Exception as e:
            print(f"âŒ æœ¬åœ°embeddingè¯·æ±‚å¤±è´¥: {e}")
            return None

    def _compute_similarity(self, query_embedding: np.ndarray) -> np.ndarray:
        """è®¡ç®—ç›¸ä¼¼åº¦"""
        # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
        similarities = np.dot(self.embeddings, query_embedding)
        norms = np.linalg.norm(self.embeddings, axis=1) * np.linalg.norm(query_embedding)
        return similarities / norms

    def generate_response(self, query: str, context: List[Dict]) -> str:
        """ç”Ÿæˆæ™ºèƒ½å“åº”"""
        if not context:
            return "æŠ±æ­‰ï¼Œæˆ‘åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚è¯·å°è¯•ä½¿ç”¨æ›´å…·ä½“çš„å…³é”®è¯ã€‚"

        # æ„å»ºä¸Šä¸‹æ–‡
        context_text = "\n\n".join([f"[æ¥æº: {c['source']}]\n{c['text']}" for c in context[:3]])

        # ä½¿ç”¨OpenAIç”Ÿæˆå“åº”
        if self.api_available["openai"] and self.config.openai_api_key:
            return self._generate_openai_response(query, context_text)
        else:
            return self._generate_rule_based_response(query, context)

    def _generate_openai_response(self, query: str, context: str) -> str:
        """ä½¿ç”¨OpenAIç”Ÿæˆå“åº”"""
        headers = {
            "Authorization": f"Bearer {self.config.openai_api_key}",
            "Content-Type": "application/json"
        }

        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”Ÿç‰©åŒ»å­¦æ•°æ®åˆ†æä¸“å®¶ï¼ŒåŸºäºFigureYaçŸ¥è¯†åº“å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

è¯·æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä¸“ä¸šã€å‡†ç¡®åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å›ç­”è¦æ±‚ï¼š
1. åŸºäºä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä¸è¦ç¼–é€ å†…å®¹
2. æä¾›å…·ä½“çš„å»ºè®®å’ŒæŒ‡å¯¼
3. ä½¿ç”¨ä¸­æ–‡å›ç­”
4. ä¿æŒä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­æ°”
5. å¦‚æœä¸Šä¸‹æ–‡ä¸è¶³ï¼Œè¯šå®åœ°è¯´æ˜å±€é™æ€§"""

        user_prompt = f"""ç”¨æˆ·é—®é¢˜ï¼š{query}

ç›¸å…³çŸ¥è¯†åº“ä¸Šä¸‹æ–‡ï¼š
{context_text}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"""

        data = {
            "model": self.config.openai_model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "max_tokens": 500,
            "temperature": 0.7
        }

        try:
            response = requests.post(
                "https://api.openai.com/v1/chat/completions",
                headers=headers,
                json=data,
                timeout=30
            )

            if response.status_code == 200:
                result = response.json()
                return result["choices"][0]["message"]["content"]
            else:
                print(f"âš ï¸ OpenAI APIè¯·æ±‚å¤±è´¥: {response.status_code}")
                return self._generate_rule_based_response(query, context)

        except Exception as e:
            print(f"âŒ OpenAIè¯·æ±‚å¤±è´¥: {e}")
            return self._generate_rule_based_response(query, context)

    def _generate_rule_based_response(self, query: str, context: List[Dict]) -> str:
        """åŸºäºè§„åˆ™çš„å“åº”ç”Ÿæˆï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰"""
        if not context:
            return "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚è¯·å°è¯•ä½¿ç”¨æ›´å…·ä½“çš„å…³é”®è¯ï¼Œæ¯”å¦‚'RNA-seqå·®å¼‚è¡¨è¾¾åˆ†æ'ã€‚"

        # æå–æœ€ç›¸å…³çš„ä¿¡æ¯
        best_match = context[0]

        response = f"""æ ¹æ®æ‚¨çš„æŸ¥è¯¢ï¼Œæˆ‘æ‰¾åˆ°äº†ä»¥ä¸‹ç›¸å…³ä¿¡æ¯ï¼š

ğŸ“Š **æ¨èæ¨¡å—**: {best_match['source']}
ğŸ¯ **ç›¸ä¼¼åº¦**: {best_match['similarity']:.2f}

**ç›¸å…³å†…å®¹**:
{best_match['text'][:300]}...

ğŸ’¡ **å»ºè®®**:
1. æ ¹æ®ä¸Šè¿°ä¿¡æ¯ï¼Œå»ºè®®æ‚¨æŸ¥çœ‹å®Œæ•´çš„æ¨¡å—æ–‡æ¡£
2. å¦‚æœéœ€è¦æ›´å…·ä½“çš„æŒ‡å¯¼ï¼Œè¯·æä¾›æ›´å¤šè¯¦ç»†ä¿¡æ¯
3. è€ƒè™‘ç»“åˆæ‚¨çš„å…·ä½“æ•°æ®ç±»å‹å’Œå®éªŒè®¾è®¡

éœ€è¦æˆ‘æä¾›æ›´å¤šè¯¦ç»†ä¿¡æ¯å—ï¼Ÿ"""

        return response

    def chat(self, query: str) -> Dict:
        """æ™ºèƒ½å¯¹è¯æ¥å£"""
        # æœç´¢ç›¸å…³çŸ¥è¯†
        search_results = self.search(query)

        # ç”Ÿæˆå“åº”
        response = self.generate_response(query, search_results)

        return {
            "query": query,
            "response": response,
            "sources": [r["source"] for r in search_results],
            "confidence": max([r["similarity"] for r in search_results]) if search_results else 0.0,
            "search_results": search_results
        }

    def get_system_info(self) -> Dict:
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        return {
            "status": "ready",
            "apis_available": self.api_available,
            "knowledge_size": len(self.text_chunks),
            "embedding_model": self.config.embedding_model if self.api_available["openai"] else self.config.local_embedding_model,
            "llm_model": self.config.openai_model if self.api_available["openai"] else "rule_based",
            "features": [
                "è¯­ä¹‰æœç´¢" if self.embeddings is not None else "å…³é”®è¯æœç´¢",
                "LLMç”Ÿæˆ" if self.api_available["openai"] else "è§„åˆ™ç”Ÿæˆ",
                "ä¸Šä¸‹æ–‡ç†è§£",
                "æ™ºèƒ½æ¨è"
            ]
        }


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§  FigureYa æ™ºèƒ½RAGç³»ç»Ÿ")
    print("=" * 40)

    # é…ç½®APIå¯†é’¥
    config = RAGConfig()
    config.openai_api_key = os.getenv("OPENAI_API_KEY", "")

    if not config.openai_api_key:
        print("âš ï¸ æœªè®¾ç½®OpenAI APIå¯†é’¥")
        print("ğŸ’¡ è¯·è®¾ç½®ç¯å¢ƒå˜é‡: export OPENAI_API_KEY='your-api-key'")
        print("ğŸ”„ å°†ä½¿ç”¨æœ¬åœ°æ¨¡å‹å’Œè§„åˆ™ç”Ÿæˆ")

    # åˆå§‹åŒ–ç³»ç»Ÿ
    rag = SmartFigureYaRAG(config)

    # åŠ è½½æˆ–æ„å»ºçŸ¥è¯†åº“
    if not rag.load_vector_store():
        rag.load_knowledge_base()
        rag.save_vector_store()

    # æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
    info = rag.get_system_info()
    print("\nğŸ“Š ç³»ç»Ÿä¿¡æ¯:")
    for key, value in info.items():
        print(f"  {key}: {value}")

    # ç¤ºä¾‹æŸ¥è¯¢
    test_queries = [
        "RNA-seqå·®å¼‚è¡¨è¾¾åˆ†æ",
        "ç”Ÿå­˜åˆ†æçš„æ–¹æ³•",
        "å•ç»†èƒè´¨é‡æ§åˆ¶",
        "å¦‚ä½•è§£é‡Šç«å±±å›¾"
    ]

    print("\nğŸ” æµ‹è¯•æŸ¥è¯¢:")
    for query in test_queries:
        print(f"\nâ“ æŸ¥è¯¢: {query}")
        result = rag.chat(query)
        print(f"ğŸ’¬ å›ç­”: {result['response'][:100]}...")
        print(f"ğŸ¯ ç½®ä¿¡åº¦: {result['confidence']:.2f}")


if __name__ == "__main__":
    main()